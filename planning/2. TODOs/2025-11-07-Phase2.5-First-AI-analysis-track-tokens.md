# Phase 2.5: AI Token Usage Tracking (Console Output)

**Date**: 2025-11-09
**Phase**: 2.5 (between Phase 2 and Phase 3)
**Status**: Planning
**Priority**: Medium

**Context**: See `2025-11-07-Phase2-First-AI-analysis-dateNtype-console.md` for the current AI implementation.

---

## Phase 2.5 Overview

Add comprehensive token usage tracking for all AI document analysis queries. This phase focuses on **accurate cost tracking** - capturing input tokens, output tokens, cached tokens, and timing information from the Gemini API. Token data is logged to console only (not displayed in UI yet).

**Scope**: Token usage capture, timestamp tracking, console logging, cost tracking foundation.

**What Phase 2 Gave Us**: Real Gemini API calls with document analysis, console logging of results.

**What Phase 2.5 Adds**: Token usage tracking, detailed timing information, foundation for cost analytics.

**What's Next**: Phase 3 will display AI results in UI; future phases may add cost dashboard.

---

## Phase 2.5 User Stories

### Story 1: Token Usage Tracking
**As a** developer
**I want to** track exact token counts for each AI query
**So that** I can accurately calculate AI costs and optimize prompt efficiency

**Acceptance Criteria**:
- Capture timestamp when prompt is sent to AI
- Capture timestamp when AI responds
- Extract `promptTokenCount` (input tokens) from API response
- Extract `candidatesTokenCount` (output tokens) from API response
- Extract `cachedContentTokenCount` (cached tokens) from API response
- Record AI model name used (e.g., "gemini-2.5-flash-lite")
- Log all token data to console in structured format
- Calculate and display elapsed time (response timestamp - prompt timestamp)

---

### Story 2: Accurate Token Counting
**As a** developer
**I want to** use the official Gemini API token counts
**So that** I have accurate billing information without estimation

**Acceptance Criteria**:
- Use `usageMetadata` from Gemini API response (not estimated counts)
- Handle responses where `cachedContentTokenCount` may be 0 or undefined
- Validate that token counts are numbers
- Log warning if `usageMetadata` is missing or malformed
- Never estimate or calculate tokens manually - always use API data

---

## Implementation Details

### Token Data Structure

Each AI analysis will track:

```javascript
{
  aiPromptSent: Timestamp,          // When request was sent
  aiResponse: Timestamp,             // When response was received
  inputTokens: number,               // promptTokenCount from API
  outputTokens: number,              // candidatesTokenCount from API
  cachedTokens: number,              // cachedContentTokenCount from API (may be 0)
  totalTokens: number,               // totalTokenCount from API
  aiModel: string,                   // e.g., "gemini-2.5-flash-lite"
  processingTime: number,            // Total time in milliseconds
  aiResponseTime: number             // Time from prompt sent to response received
}
```

---

### Update aiMetadataExtractionService.js

**Location**: `src/services/aiMetadataExtractionService.js`

**Changes to `analyzeDocument()` method**:

```javascript
async analyzeDocument(base64Data, evidence, extension = 'pdf', firmId, matterId = 'general') {
  const startTime = Date.now();

  // NEW: Create timestamp for when prompt is sent
  let aiPromptSent = null;
  let aiResponse = null;

  try {
    console.log('> Starting Gemini AI analysis...');
    console.log('File:', evidence.displayName || 'Unknown');
    console.log('Extension:', extension);
    console.log('Size:', this._formatFileSize(evidence.fileSize));

    if (!firebaseAI) {
      throw new Error('Firebase AI Logic not initialized. Ensure VITE_ENABLE_AI_FEATURES is set to true.');
    }

    // Get Gemini model
    const modelName = 'gemini-2.5-flash-lite';
    const model = getGenerativeModel(firebaseAI, { model: modelName });

    // Fetch document types
    const documentTypes = await this._getDocumentTypes(firmId, matterId);
    console.log('=Ë Document types loaded:', documentTypes.length, 'types');

    const prompt = this._buildPrompt(documentTypes);
    const mimeType = this._getMimeType(extension);

    console.log('=ä Sending request to Gemini API...');

    // NEW: Record timestamp when sending prompt
    aiPromptSent = Date.now();

    // Generate AI response
    const result = await model.generateContent([
      { text: prompt },
      { inlineData: { mimeType: mimeType, data: base64Data } },
    ]);

    const response = await result.response;

    // NEW: Record timestamp when response received
    aiResponse = Date.now();

    const text = response.text();

    console.log('=å Received response from Gemini API');
    console.log('Raw response:', text);

    // NEW: Extract token usage from response
    const usageMetadata = response.usageMetadata || {};
    const tokenUsage = {
      inputTokens: usageMetadata.promptTokenCount || 0,
      outputTokens: usageMetadata.candidatesTokenCount || 0,
      cachedTokens: usageMetadata.cachedContentTokenCount || 0,
      totalTokens: usageMetadata.totalTokenCount || 0,
      aiModel: modelName,
      aiPromptSent: new Date(aiPromptSent).toISOString(),
      aiResponse: new Date(aiResponse).toISOString(),
      aiResponseTime: aiResponse - aiPromptSent
    };

    // NEW: Validate token data
    if (!usageMetadata || typeof usageMetadata.promptTokenCount !== 'number') {
      console.warn('  usageMetadata missing or malformed in API response');
      console.warn('Response object:', response);
    }

    // Parse response
    const parsedResults = this._parseResponse(text);

    const processingTime = Date.now() - startTime;

    // NEW: Log comprehensive token usage
    console.log('\n=Ê ===== TOKEN USAGE REPORT =====');
    console.log('AI Model:', tokenUsage.aiModel);
    console.log('Prompt Sent:', tokenUsage.aiPromptSent);
    console.log('Response Received:', tokenUsage.aiResponse);
    console.log('AI Response Time:', tokenUsage.aiResponseTime, 'ms');
    console.log('\n=° Token Counts:');
    console.log('  Input Tokens:', tokenUsage.inputTokens);
    console.log('  Output Tokens:', tokenUsage.outputTokens);
    console.log('  Cached Tokens:', tokenUsage.cachedTokens);
    console.log('  Total Tokens:', tokenUsage.totalTokens);
    console.log('================================\n');

    console.log(` Analysis completed in ${processingTime}ms`);
    console.log('Parsed results:', parsedResults);

    return {
      ...parsedResults,
      processingTime,
      tokenUsage  // NEW: Include token usage in return value
    };
  } catch (error) {
    console.error('L AI analysis failed:', error);
    throw error;
  }
}
```

---

### Console Output Format

After successful AI analysis, console will display:

```
> Starting Gemini AI analysis...
File: invoice-march-2024.pdf
Extension: pdf
Size: 234.56 KB
=Ë Document types loaded: 35 types
=ä Sending request to Gemini API...
=å Received response from Gemini API
Raw response: {"documentDate": {...}, "documentType": {...}}

=Ê ===== TOKEN USAGE REPORT =====
AI Model: gemini-2.5-flash-lite
Prompt Sent: 2025-11-09T14:32:15.234Z
Response Received: 2025-11-09T14:32:17.589Z
AI Response Time: 2355 ms

=° Token Counts:
  Input Tokens: 2845
  Output Tokens: 156
  Cached Tokens: 0
  Total Tokens: 3001
================================

 Analysis completed in 2456ms
Parsed results: { documentDate: {...}, documentType: {...} }
```

---

## Gemini API Token Usage - Research Findings

### Official API Documentation

Based on research conducted 2025-11-09:

**Source**: Google AI for Developers, Firebase AI Logic documentation

**Token Metadata Structure**:

The Gemini API response includes a `usageMetadata` object with the following properties:

```javascript
{
  promptTokenCount: number,           // Input tokens (prompt + file content)
  candidatesTokenCount: number,       // Output tokens (AI response)
  cachedContentTokenCount: number,    // Cached input tokens (optional, may be 0)
  totalTokenCount: number             // Total = prompt + candidates
}
```

**Property Names**:
- JavaScript/TypeScript: Uses **camelCase** (e.g., `promptTokenCount`)
- Python: Uses **snake_case** (e.g., `prompt_token_count`)

**Access Pattern** (Firebase AI Logic - JavaScript):

```javascript
const result = await model.generateContent([...]);
const response = await result.response;

// Access token counts
const usageMetadata = response.usageMetadata;
console.log('Input tokens:', usageMetadata.promptTokenCount);
console.log('Output tokens:', usageMetadata.candidatesTokenCount);
console.log('Cached tokens:', usageMetadata.cachedContentTokenCount);
console.log('Total tokens:', usageMetadata.totalTokenCount);
```

**Context Caching** (Gemini 2.5+):

- **Implicit caching enabled by default** for all Gemini 2.5 models (effective May 8, 2025)
- Cached tokens automatically appear in `cachedContentTokenCount`
- Cached tokens cost **10% of standard input token cost** (90% savings)
- Minimum input tokens for caching:
  - Gemini 2.5 Flash: 1,024 tokens
  - Gemini 2.5 Pro: 4,096 tokens

**Streaming Responses**:

- For streaming, all chunks have empty `usageMetadata: {}`
- **Only the final chunk** contains complete token statistics

**Model Used**:

Current implementation uses: `gemini-2.5-flash-lite`
- Context window: 1,048,576 tokens (1M)
- Max output: 65,536 tokens
- Supports implicit caching
- All Gemini 1.0 and 1.5 models retired September 2025

---

## Success Criteria

### Functional
- **F1**: Timestamp recorded when prompt sent to AI
- **F2**: Timestamp recorded when AI responds
- **F3**: Input tokens extracted from `usageMetadata.promptTokenCount`
- **F4**: Output tokens extracted from `usageMetadata.candidatesTokenCount`
- **F5**: Cached tokens extracted from `usageMetadata.cachedContentTokenCount`
- **F6**: Model name recorded correctly (e.g., "gemini-2.5-flash-lite")
- **F7**: Token usage logged to console in clear, structured format
- **F8**: AI response time calculated correctly (response - prompt timestamps)

### Data Quality
- **D1**: All token counts are actual numbers (not null/undefined)
- **D2**: Cached tokens default to 0 if not present in response
- **D3**: Timestamps are valid ISO 8601 strings
- **D4**: Total tokens match (input + output) when cached = 0
- **D5**: Warning logged if usageMetadata is missing or malformed

### Console Output
- **C1**: Token report clearly separated with header/footer
- **C2**: All timestamps shown in ISO 8601 format
- **C3**: Response time shown in milliseconds
- **C4**: Token counts aligned and easy to read
- **C5**: Model name displayed prominently

---

## Testing Plan

### Unit Tests

**File**: `tests/unit/services/aiMetadataExtractionService.test.js`

**New Tests to Add**:

```javascript
describe('Token Usage Tracking', () => {
  it('extracts token counts from API response', async () => {
    // Mock API response with usageMetadata
    const mockResponse = {
      text: () => '{"documentDate": {...}, "documentType": {...}}',
      usageMetadata: {
        promptTokenCount: 2845,
        candidatesTokenCount: 156,
        cachedContentTokenCount: 0,
        totalTokenCount: 3001
      }
    };

    // Test that service extracts token data correctly
    const result = await aiMetadataExtractionService.analyzeDocument(
      'base64data',
      { displayName: 'test.pdf', fileSize: 1024 },
      'pdf',
      'firm123',
      'matter456'
    );

    expect(result.tokenUsage.inputTokens).toBe(2845);
    expect(result.tokenUsage.outputTokens).toBe(156);
    expect(result.tokenUsage.cachedTokens).toBe(0);
    expect(result.tokenUsage.totalTokens).toBe(3001);
    expect(result.tokenUsage.aiModel).toBe('gemini-2.5-flash-lite');
  });

  it('handles cached tokens correctly', async () => {
    const mockResponse = {
      text: () => '{"documentDate": {...}, "documentType": {...}}',
      usageMetadata: {
        promptTokenCount: 2845,
        candidatesTokenCount: 156,
        cachedContentTokenCount: 2500,  // Some tokens cached
        totalTokenCount: 3001
      }
    };

    const result = await aiMetadataExtractionService.analyzeDocument(...);
    expect(result.tokenUsage.cachedTokens).toBe(2500);
  });

  it('defaults cachedTokens to 0 if not present', async () => {
    const mockResponse = {
      text: () => '{"documentDate": {...}, "documentType": {...}}',
      usageMetadata: {
        promptTokenCount: 2845,
        candidatesTokenCount: 156,
        // No cachedContentTokenCount
        totalTokenCount: 3001
      }
    };

    const result = await aiMetadataExtractionService.analyzeDocument(...);
    expect(result.tokenUsage.cachedTokens).toBe(0);
  });

  it('validates timestamps are ISO 8601 format', async () => {
    const result = await aiMetadataExtractionService.analyzeDocument(...);

    // Should be valid ISO 8601 strings
    expect(result.tokenUsage.aiPromptSent).toMatch(/^\d{4}-\d{2}-\d{2}T/);
    expect(result.tokenUsage.aiResponse).toMatch(/^\d{4}-\d{2}-\d{2}T/);

    // Should be parseable as dates
    expect(() => new Date(result.tokenUsage.aiPromptSent)).not.toThrow();
    expect(() => new Date(result.tokenUsage.aiResponse)).not.toThrow();
  });

  it('calculates response time correctly', async () => {
    const result = await aiMetadataExtractionService.analyzeDocument(...);

    const promptTime = new Date(result.tokenUsage.aiPromptSent).getTime();
    const responseTime = new Date(result.tokenUsage.aiResponse).getTime();
    const expectedDiff = responseTime - promptTime;

    expect(result.tokenUsage.aiResponseTime).toBe(expectedDiff);
    expect(result.tokenUsage.aiResponseTime).toBeGreaterThan(0);
  });

  it('logs warning when usageMetadata is missing', async () => {
    const consoleWarnSpy = vi.spyOn(console, 'warn');

    const mockResponse = {
      text: () => '{"documentDate": {...}, "documentType": {...}}',
      // No usageMetadata
    };

    await aiMetadataExtractionService.analyzeDocument(...);

    expect(consoleWarnSpy).toHaveBeenCalledWith(
      expect.stringContaining('usageMetadata missing or malformed')
    );
  });
});
```

---

### Manual Test Cases

#### TC1: Normal Analysis with Token Tracking
**File**: Standard invoice PDF
**Expected Console Output**:
```
> Starting Gemini AI analysis...
File: invoice-march-2024.pdf
Extension: pdf
Size: 234.56 KB
=Ë Document types loaded: 35 types
=ä Sending request to Gemini API...
=å Received response from Gemini API
Raw response: {...}

=Ê ===== TOKEN USAGE REPORT =====
AI Model: gemini-2.5-flash-lite
Prompt Sent: 2025-11-09T14:32:15.234Z
Response Received: 2025-11-09T14:32:17.589Z
AI Response Time: 2355 ms

=° Token Counts:
  Input Tokens: 2845
  Output Tokens: 156
  Cached Tokens: 0
  Total Tokens: 3001
================================

 Analysis completed in 2456ms
```

#### TC2: Analysis with Cached Tokens
**File**: PDF analyzed shortly after similar document (cache hit)
**Expected**: `Cached Tokens: 2500` (non-zero value)

#### TC3: Large Document Analysis
**File**: 15MB complex PDF
**Expected**: Higher token counts (input tokens > 10,000)

---

## Cost Calculation Reference

### Gemini 2.5 Flash Pricing (as of 2025)

**Note**: This section is for reference only. Phase 2.5 does NOT calculate costs in dollars - just raw token counts.

**Standard Rates**:
- Input tokens: $0.075 per 1M tokens ($0.000000075 per token)
- Output tokens: $0.30 per 1M tokens ($0.0000003 per token)
- Cached input tokens: $0.0075 per 1M tokens ($0.0000000075 per token) - 90% discount

**Example Calculation** (for 1 query):
```
Input tokens: 2,845 × $0.000000075 = $0.00021338
Output tokens: 156 × $0.0000003 = $0.0000468
Cached tokens: 0 × $0.0000000075 = $0
Total cost: $0.00026018 (~$0.26 per 1,000 similar queries)
```

**Future Enhancement**: A later phase may add cost calculation and dashboard.

---

## Environment Variables

No new environment variables needed.

Existing variables:
```env
VITE_ENABLE_AI_FEATURES=true
VITE_AI_MAX_FILE_SIZE_MB=20
```

---

## Dependencies

### External APIs
- Google Gemini 2.5 Flash Lite via Firebase AI Logic
- No changes to existing API dependencies

### NPM Packages
- `firebase` (already installed)
- `firebase/ai` (already in use)

### Internal Services
- `aiMetadataExtractionService.js` (update existing)
- No new services needed

---

## Notes

### Phase 2.5 Behavior
- Token tracking runs automatically with every AI analysis 
- Token data logged to console 
- Token data included in service return value 
- **Token data NOT displayed in UI**   (console only)
- **Costs NOT calculated**   (raw token counts only)

### Why Track Tokens Now?

1. **Cost Visibility**: Understand actual AI costs vs. estimates
2. **Optimization**: Identify expensive queries, optimize prompts
3. **Billing**: Accurate cost tracking for client billing (future)
4. **Analytics**: Foundation for usage dashboards (future)
5. **Budgeting**: Set realistic AI budget based on real usage

### Context Caching Impact

With implicit caching enabled (Gemini 2.5):
- Repeated similar queries may show high `cachedTokens`
- Cached tokens cost 90% less than input tokens
- Expect cache hits when:
  - Analyzing multiple pages from same document
  - Analyzing similar document types consecutively
  - Re-analyzing same document

### Accuracy vs. Estimation

**IMPORTANT**: This implementation uses **actual API token counts**, not estimates:

 **Correct**: Extract from `response.usageMetadata`
L **Wrong**: Count characters/words and estimate

Gemini API provides exact token counts used for billing. Always use these official counts.

---

## Future Enhancements (Not Phase 2.5)

### Phase 3+: Potential Features
- [ ] Display token usage in UI (developer mode toggle)
- [ ] Store token usage in Firestore for analytics
- [ ] Calculate and display costs in dollars
- [ ] Usage dashboard with charts
- [ ] Monthly cost summaries
- [ ] Cost alerts when approaching budget limits
- [ ] Prompt optimization recommendations
- [ ] Comparison of token costs across document types

---

## Completion Checklist

- [ ] `aiMetadataExtractionService.js` updated with token tracking
- [ ] `aiPromptSent` timestamp captured correctly
- [ ] `aiResponse` timestamp captured correctly
- [ ] `inputTokens` extracted from `usageMetadata.promptTokenCount`
- [ ] `outputTokens` extracted from `usageMetadata.candidatesTokenCount`
- [ ] `cachedTokens` extracted from `usageMetadata.cachedContentTokenCount`
- [ ] `totalTokens` extracted from `usageMetadata.totalTokenCount`
- [ ] `aiModel` recorded correctly
- [ ] Console logging formatted clearly with emoji indicators
- [ ] Warning logged when `usageMetadata` is missing
- [ ] Default value (0) for `cachedTokens` when not present
- [ ] Unit tests added for token tracking (6 new tests)
- [ ] Manual tests pass (TC1-TC3)
- [ ] Token counts validated as numbers
- [ ] Timestamps validated as ISO 8601 strings
- [ ] Response time calculated correctly
- [ ] No regressions in existing functionality
- [ ] Documentation updated with API research findings
- [ ] Ready for Phase 3 (UI integration)

---

## Implementation Notes

**Status**: Planning
**Date Created**: 2025-11-09
**Expected Implementation**: TBD

### Research Summary

**Date Researched**: 2025-11-09

**Key Findings**:
1. Gemini API provides exact token counts in `response.usageMetadata`
2. Property names use camelCase in JavaScript (e.g., `promptTokenCount`)
3. Gemini 2.5 models have implicit caching enabled by default (since May 2025)
4. Cached tokens appear in `cachedContentTokenCount` (may be 0)
5. For streaming responses, only final chunk has token metadata
6. Current model (`gemini-2.5-flash-lite`) supports caching with 1,024 token minimum

**Documentation Sources**:
- Firebase AI Logic documentation (firebase.google.com/docs/ai-logic)
- Google AI for Developers (ai.google.dev/gemini-api)
- Firebase JavaScript API reference
- Google Developers Blog (Gemini 2.5 caching announcement)

**No Estimation Needed**: Gemini API provides exact billing tokens - no need for character/word counting or estimation algorithms.

---

## Integration with Existing Phases

**Builds On**:
- Phase 2: Real AI integration with console logging

**Prepares For**:
- Phase 3: UI display of AI results (can include token usage in dev mode)
- Future phases: Cost analytics, usage dashboards

**Does Not Conflict With**:
- Any existing or planned phases
- Can be implemented independently
- Non-breaking change (additive only)

---

## Risk Assessment

**Risks**: Low

**Potential Issues**:
1. **API Changes**: Google could change `usageMetadata` structure
   - *Mitigation*: Defensive coding with defaults, validation, warnings
2. **Missing Metadata**: Some responses might not include `usageMetadata`
   - *Mitigation*: Default to 0, log warning, continue processing
3. **Performance**: Minimal impact (just extracting existing data)
   - *Mitigation*: No performance concerns - data already in response

**Breaking Changes**: None
- Existing code continues to work
- Token tracking is additive only
- Return value extended (not changed)

---

## Questions for Implementation

**Answered Questions**:
-  Q: What are the exact property names? A: `promptTokenCount`, `candidatesTokenCount`, `cachedContentTokenCount`
-  Q: Does Gemini 2.5 Flash support caching? A: Yes, implicit caching enabled by default
-  Q: Should we estimate tokens? A: No, use official API counts only
-  Q: Where should token data be logged? A: Console only (Phase 2.5), not UI yet

**Open Questions**:
- S Should token usage be stored in Firestore? (Defer to Phase 3+)
- S Should we calculate costs in dollars? (Defer to Phase 3+)
- S Should there be a developer mode toggle for UI display? (Defer to Phase 3+)

---
